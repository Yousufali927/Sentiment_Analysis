{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67aeb705",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e59772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5839b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = 'txt_sentoken'\n",
    "\n",
    "# Dictionary to store file contents\n",
    "file_contents_dict = {}\n",
    "\n",
    "for subfolder_name in os.listdir(parent_folder):\n",
    "    subfolder_path = os.path.join(parent_folder, subfolder_name)\n",
    "\n",
    "    # To check if the subfolder is a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        # List to store file contents in the subfolder\n",
    "        subfolder_contents = []\n",
    "\n",
    "        # Iterate through each file in the subfolder\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            \n",
    "            if filename.endswith('.txt'):\n",
    "                # Construct the path to the file\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "\n",
    "                # Read the contents of the file and append to subfolder_contents list\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_contents = file.read()\n",
    "                    subfolder_contents.append(file_contents)\n",
    "\n",
    "        file_contents_dict[subfolder_name] = subfolder_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Data Format\n",
    "for subfolder_name, contents in file_contents_dict.items():\n",
    "    print(f\"First 20 contents of files in folder {subfolder_name}:\")\n",
    "    for i, content in enumerate(contents[:2], start=1):\n",
    "        print(f\"File {i}:\")\n",
    "        print(content)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing reviews in variables\n",
    "neg_reviews = file_contents_dict['neg']\n",
    "pos_reviews = file_contents_dict['pos']\n",
    "all_reviews = neg_reviews + pos_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negative = 1000\n",
    "num_positive = 1000\n",
    "\n",
    "# Creating labels for the model to be used for learning\n",
    "labels = [0] * num_negative + [1] * num_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace34ea",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Download NLTK resources (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer and set of stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess a list of documents\n",
    "def preprocess_documents(documents):\n",
    "    preprocessed_documents = []\n",
    "    for doc in documents:\n",
    "        # Remove HTML tags\n",
    "        doc = re.sub(r'<.*?>', '', doc)\n",
    "        \n",
    "        # Remove non-alphanumeric characters and extra whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc)\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(doc)\n",
    "        \n",
    "        # Lowercasing\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        \n",
    "        # Stopword removal\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Stemming\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "        preprocessed_documents.append(tokens)\n",
    "    \n",
    "    return preprocessed_documents\n",
    "\n",
    "# Concatenate negative and positive reviews into a single list\n",
    "all_reviews = neg_reviews + pos_reviews\n",
    "\n",
    "preprocessed_all_reviews = preprocess_documents(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0192e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=preprocessed_all_reviews, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"word2vec_model\")\n",
    "\n",
    "# Optionally, you can also save the vectors\n",
    "model.wv.save_word2vec_format(\"word_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Loading the pre-trained Word2Vec model\n",
    "model = KeyedVectors.load_word2vec_format(\"word_vectors.txt\", binary=False)\n",
    "\n",
    "# Function to convert text to word vectors\n",
    "def text_to_vectors(text):\n",
    "    word_vectors = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            vector = model[word]  \n",
    "            word_vectors.append(vector)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return word_vectors\n",
    "\n",
    "# Apply function to the preprocessed text data\n",
    "word_vectors = [text_to_vectors(text) for text in preprocessed_all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle the data and labels in unison\n",
    "combined_data = list(zip(word_vectors, labels))\n",
    "random.shuffle(combined_data)\n",
    "word_vectors_shuffled, labels_shuffled = zip(*combined_data)\n",
    "\n",
    "# Split the shuffled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_vectors_shuffled, labels_shuffled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7318e5",
   "metadata": {},
   "source": [
    "## ^^^ Above ^^^\n",
    "\n",
    "Since the data we have was separate, we do not want to end up training the model on more of one type of reviews, above code make sure that doesnt happen by shuffling the features and respective labels together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average word vectors for each review\n",
    "def average_word_vectors(word_vectors):\n",
    "    averaged_vectors = []\n",
    "    for review_vectors in word_vectors:\n",
    "        if len(review_vectors) > 0:\n",
    "            averaged_vector = np.mean(review_vectors, axis=0)\n",
    "        else:\n",
    "            averaged_vector = np.zeros_like(word_vectors[0][0])  # If the review has no word vectors, use zeros\n",
    "        averaged_vectors.append(averaged_vector)\n",
    "    return averaged_vectors\n",
    "\n",
    "# Average word vectors for training data\n",
    "X_train_averaged = average_word_vectors(X_train)\n",
    "\n",
    "# Average word vectors for testing data\n",
    "X_test_averaged = average_word_vectors(X_test)\n",
    "\n",
    "\n",
    "## Averaging out i.e pooling the word vectors into one to feed to the model\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_averaged, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test_averaged)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ad331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a11e42c",
   "metadata": {},
   "source": [
    "## Trying Using OpenAI Embeddings\n",
    "\n",
    "instead of using word2vec for the vector representation of words, we can also use other embeddings, but we will still finally have to pool the vectors into one because the ml models only takes one input vector, we lose of a lot of information because of this, not being able to process the words of the sentence in the same order or opposite words negating some affects of each other in the pooled vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aac30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb43eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create a list to store review embeddings\n",
    "review_embeddings = []\n",
    "\n",
    "# trying to pool the vectors for each word into one to pass to the ML model.\n",
    "def average_pooling(embeddings):\n",
    "    \"\"\"Calculates the average of the word embeddings in a document.\"\"\"\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Function to get embeddings for a single review\n",
    "def get_embeddings(review):\n",
    "    # Get embeddings for each token\n",
    "    token_embeddings = embeddings.embed_documents(review)\n",
    "    # Pool the token embeddings (here, using average pooling)\n",
    "    pooled_review_embedding = average_pooling(token_embeddings)\n",
    "    return pooled_review_embedding\n",
    "\n",
    "# Generate embeddings for each cleaned review\n",
    "for review in preprocessed_all_reviews:\n",
    "    embedding = get_embeddings(review)\n",
    "    review_embeddings.append(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
